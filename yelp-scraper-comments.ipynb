{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from requests import get\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time \n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('./data/restaurant_ratings.csv')\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted columns\n",
    "data = data_raw.drop(['Unnamed: 0', \n",
    "                     'Rating', \n",
    "                     'Number of Ratings',\n",
    "                     'Affordability'],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add https:// in front of the review links\n",
    "data['Reviews Link'] = 'https://www.' + data['Reviews Link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if length is correct\n",
    "len(list(data[\"Reviews Link\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store links as a list\n",
    "LIST_OF_LINKS = list(data[\"Reviews Link\"])\n",
    "# show first 5 links\n",
    "LIST_OF_LINKS[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start value for every new page in yelp\n",
    "start_of_new_page = [str(i) for i in range(0,381,20)]\n",
    "# debug\n",
    "print(start_of_new_page, end=' ')\n",
    "print(f'\\nNumber of pages scraped: {len(start_of_new_page)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(list_of_links, num_req, start, end):\n",
    "    # redeclaring lists to store data in multiple values\n",
    "    cust_names = []\n",
    "    cust_ratings = []\n",
    "    cust_comments = []\n",
    "    res_names = []\n",
    "    res_types = []\n",
    "\n",
    "    # counter\n",
    "    count = 0\n",
    "\n",
    "    # flag variable to check the scrape\n",
    "    # if unsuccessful scrape, try again\n",
    "    unsuccessful = True\n",
    "\n",
    "    # preparing the monitoring of the loop\n",
    "    start_time = time()\n",
    "\n",
    "    ### ----- \n",
    "\n",
    "    # for every comment in the interval\n",
    "    for link_raw in list_of_links[start:end+1]:\n",
    "        loop_time = time()\n",
    "        count+=1 # increment count to determine which link it is being scraped\n",
    "        requests=1 # reset requests count for different webpage\n",
    "        # print(f'----- LINK {count} -----')\n",
    "        for pageStart in start_of_new_page:\n",
    "\n",
    "            # Break the loop if the number of requests is greater than expected\n",
    "            if requests > num_req:\n",
    "                #warn('Number of requests was greater than expected.')\n",
    "                break\n",
    "\n",
    "            unsuccessful = True\n",
    "            fail_count = 0\n",
    "            repeat = 0\n",
    "\n",
    "            while unsuccessful:\n",
    "                # make a get request\n",
    "                #response = get(f'https://www.yelp.com/biz/jumbo-seafood-singapore-4?start={pageStart}')\n",
    "                #response = get(f'https://www.tripadvisor.com.sg/Restaurant_Review-g294265-d7348336-Reviews-or{pageStart}-Sunday_Folks-Singapore.html')\n",
    "                \n",
    "                #link_array = link_raw.split('Reviews-')\n",
    "                link = link_raw + '&start=' + str(pageStart)\n",
    "                # print(link)\n",
    "                response = get(link)\n",
    "\n",
    "                # pause the loop\n",
    "                sleep(randint(3,7))\n",
    "\n",
    "                # monitor the requests\n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'LINK {count+start} REQUEST {requests}; Frequency: {requests/elapsed_time} requests/s')\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # get the comment container for all 20 comments in a page\n",
    "                comment_containers = page_html.find_all('div', class_='review__373c0__13kpL border-color--default__373c0__3-ifU')\n",
    "\n",
    "                if len(comment_containers) != 0:\n",
    "                    print(f\"REQUEST {requests}: SUCCESS --> Failed Count: {fail_count}\")\n",
    "                    clear_output(wait = True)\n",
    "                    unsuccessful = False\n",
    "                else:\n",
    "                    fail_count+=1\n",
    "                    repeat+=1\n",
    "                    #print(f\"Request {requests}: unsuccessful scrape\") # debug\n",
    "                if repeat >= 8:\n",
    "                    print(\"Repeated 8 times --> cannot scrape\")\n",
    "                    break\n",
    "\n",
    "            requests += 1\n",
    "\n",
    "            # for every comments in 10\n",
    "            for com in comment_containers:\n",
    "                # in case the scrape fail for that particular entry due to html tag issue\n",
    "                try:\n",
    "                    # append the restaurant name and type\n",
    "                    res_names.append(data[\"Restaurant\"][count+start-1])\n",
    "                    res_types.append(data[\"Restaurant Type\"][count+start-1])\n",
    "\n",
    "                    # scrape the customer name\n",
    "                    cust_name = com.div.find('a', class_='link__373c0__1G70M link-color--inherit__373c0__3dzpk link-size--inherit__373c0__1VFlE').text\n",
    "                    cust_names.append(cust_name)\n",
    "\n",
    "                    # scrape the customer ratings\n",
    "                    cust_rating = com.find('div', class_='arrange__373c0__2C9bH gutter-1__373c0__2l5bx vertical-align-middle__373c0__1SDTo border-color--default__373c0__3-ifU').span.div['aria-label']\n",
    "                    cust_ratings.append(cust_rating)\n",
    "\n",
    "                    cust_comment_raw = com.find_all('div', class_='margin-b2__373c0__abANL border-color--default__373c0__3-ifU')\n",
    "                    if len(cust_comment_raw) != 1:\n",
    "                        temp = cust_comment_raw[1].text\n",
    "                    else:\n",
    "                        temp = cust_comment_raw[0].text\n",
    "\n",
    "                    cust_comment = temp.replace(u'\\xa0', u'')\n",
    "                    cust_comments.append(cust_comment)\n",
    "                except:\n",
    "                    print(f'error in request {requests}')\n",
    "                    continue\n",
    "\n",
    "            # Throw a warning for non-200 status codes\n",
    "            if response.status_code != 200:\n",
    "                warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        # check time needed to exceute one link\n",
    "        print(f'Time taken for scraping link {count+start}: {time() - loop_time} seconds')\n",
    "\n",
    "    print('DONE')\n",
    "    return cust_names, cust_ratings, cust_comments, res_names, res_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper call function\n",
    "def scraper_call(list_of_links, num_req, start, end):\n",
    "    cust_names, cust_ratings, cust_comments, res_names, res_types = scraper(list_of_links = list_of_links, \n",
    "                                                                            num_req=num_req, \n",
    "                                                                            start=start, \n",
    "                                                                            end=end)\n",
    "    print(f'Number of entries: {len(cust_names)}')\n",
    "    review = pd.DataFrame({\n",
    "        'Restaurant Name': res_names,\n",
    "        'Restaurant Type': res_types,\n",
    "        'Reviewer\\'s Name': cust_names,\n",
    "        'Rating': cust_ratings,\n",
    "        'Comment': cust_comments,\n",
    "    })\n",
    "\n",
    "    print(review.info())\n",
    "    if start == 0:\n",
    "        review.to_csv('./data/yelp-comments.csv', mode='a', index=False)\n",
    "    else:\n",
    "        review.to_csv('./data/yelp-comments.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number of requests (each request is 10 entries)\n",
    "REQUESTS = 20\n",
    "start_list = [s for s in range(0,221,50)]\n",
    "end_list = [e for e in range(49,221,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test values\n",
    "print(start_list)\n",
    "print(end_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALLING OF FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 0 - 49\n",
    "scraper_call(list_of_links=LIST_OF_LINKS, num_req=REQUESTS, start=start_list[0], end=end_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 50 - 99\n",
    "scraper_call(list_of_links=LIST_OF_LINKS, num_req=REQUESTS, start=start_list[1], end=end_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 100 - 149\n",
    "scraper_call(list_of_links=LIST_OF_LINKS, num_req=REQUESTS, start=start_list[2], end=end_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 150 - 199\n",
    "scraper_call(list_of_links=LIST_OF_LINKS, num_req=REQUESTS, start=start_list[3], end=end_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 200 - 220\n",
    "scraper_call(list_of_links=LIST_OF_LINKS, num_req=REQUESTS, start=start_list[4], end=219)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
